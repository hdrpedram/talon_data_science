{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis of Metro Interstate Traffic Volume\n",
    "\n",
    "This notebook explores the Metro Interstate Traffic Volume dataset, which includes hourly traffic volume data along with weather and holiday information from 2012 to 2018 on Interstate 94 Westbound in Minneapolis-St Paul, MN. The goal is to perform exploratory data analysis to understand traffic patterns and their relationship with environmental conditions and holidays.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Source:** https://archive.ics.uci.edu/dataset/492/metro+interstate+traffic+volume"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](data/metadata_desc.JPG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Data Loading\n",
    "\n",
    "### Import libraries and loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('data/modified_Metro_traffic_data.csv')\n",
    "# Replace NaN values in 'holiday' column with the string 'None'\n",
    "#data['holiday'].fillna('None', inplace=True)\n",
    "data.fillna({'holiday': 'None'}, inplace=True)\n",
    "\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Data Exploration\n",
    "\n",
    "### Examining the data structure, types of data, unique values, and basic statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General overview of the data structure\n",
    "print(\"Data Shape:\", data.shape)  # Showing the number of rows and columns/features\n",
    "print(\"\\nData Info:\")\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overview of numerical features\n",
    "print(\"\\nStatistics for Numerical Features:\")\n",
    "print(data.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overview of categorical features\n",
    "categorical_cols = data.select_dtypes(include=['object']).columns\n",
    "print(\"\\nCategorical Features:\")\n",
    "for col in categorical_cols:\n",
    "    print(f\"--- {col} ---\")\n",
    "    print(data[col].value_counts())\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visual overview using histograms for numerical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual overview using histograms for numerical data\n",
    "data.hist(figsize=(12, 10))\n",
    "plt.suptitle('Histograms of Numerical Features')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boxplots for each numerical feature to spot outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplots for each numerical feature to spot outliers\n",
    "fig, axs = plt.subplots(nrows=len(data.select_dtypes(include=['number']).columns), figsize=(8, 20))\n",
    "for i, col in enumerate(data.select_dtypes(include=['number']).columns):\n",
    "    sns.boxplot(x=data[col], ax=axs[i])\n",
    "    axs[i].set_title(f'Boxplot of {col}')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "\n",
    "### Handle missing values, remove duplicates, and correct data types if necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(data.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Data Shape-Original:\", data.shape)  \n",
    "\n",
    "# Remove duplicates\n",
    "data.drop_duplicates(inplace=True)\n",
    "\n",
    "print(\"Data Shape-After removing dupliocates:\", data.shape)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correcting data types: Making Sure date-time is represented correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correct data types\n",
    "data['date_time'] = pd.to_datetime(data['date_time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis\n",
    "\n",
    "### Statistical summary and visualize data distributions.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical summary\n",
    "print(data.describe())\n",
    "\n",
    "# Distribution of traffic volumes\n",
    "sns.histplot(data['traffic_volume'], kde=True)\n",
    "plt.title('Distribution of Traffic Volume')\n",
    "plt.show()\n",
    "\n",
    "# Boxplot for hourly traffic volume\n",
    "sns.boxplot(x=data['date_time'].dt.hour, y='traffic_volume', data=data)\n",
    "plt.title('Hourly Traffic Volume')\n",
    "plt.xlabel('Hour of the Day')\n",
    "plt.ylabel('Traffic Volume')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identifying Noise and Anomalies\n",
    "\n",
    "**Pairplot:** This plot helps in quickly spotting distributions, anomalies, and relationships between multiple numerical variables. Skewed distributions or unusual scatter patterns can suggest outliers or anomalies.\n",
    "\n",
    "**Correlation Heatmap:** Useful for identifying relationships between variables. Highly correlated variables or unexpected correlations can suggest underlying patterns or errors in data collection.\n",
    "\n",
    "**Count Plots for Categorical Data:** These plots are excellent for visualizing the frequency distribution of categorical variables. Anomalies might be very rare categories that could actually be data entry errors.\n",
    "\n",
    "**Boxplots for Each Numerical Feature:** Boxplots are particularly useful for spotting outliers. They provide a clear visualization of the quartile ranges and any points that fall outside these ranges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplots for numerical features to identify outliers\n",
    "fig, axs = plt.subplots(nrows=len(data.select_dtypes(include=['number']).columns), figsize=(8, 20))\n",
    "for i, col in enumerate(data.select_dtypes(include=['number']).columns):\n",
    "    sns.boxplot(x=data[col], ax=axs[i])\n",
    "    axs[i].set_title(f'Boxplot of {col} - Check for Outliers')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Removing Negative Values:** This step filters out entries with negative traffic volumes, which do not make sense in this context and should be considered noise or errors.\n",
    "\n",
    "**Handling Outliers:** Traffic volumes that are too large are treated as outliers based on the interquartile range (IQR) method. This helps in normalizing the data distribution and preparing it for more accurate model predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
    "# Removing rows with negative traffic volume\n",
    "data = data[data['traffic_volume'] >= 0]\n",
    "\n",
    "# Removing extreme outliers in traffic volume\n",
    "Q1 = data['traffic_volume'].quantile(0.25)\n",
    "Q3 = data['traffic_volume'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "data = data[(data['traffic_volume'] >= lower_bound) & (data['traffic_volume'] <= upper_bound)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New Distribution of traffic volumes\n",
    "sns.histplot(data['traffic_volume'], kde=True)\n",
    "plt.title('Distribution of Traffic Volume')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a question: How should we start transforming our dataset into a format that is efficiently read by an ML model? How can we scale this transformation so that we do not need to repeat it for data ingestion every time?\n",
    "\n",
    "Using a **transformation pipeline** is the answer, especially when you plan to scale your project to handle new incoming data for prediction. A transformation pipeline automates the steps of data preprocessing, such as scaling, encoding, and handling date-time variables. This not only ensures consistency in how data is treated both during model training and prediction but also simplifies the process of applying the same transformations to new data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up a Transformation Pipeline\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "from scipy.sparse import issparse\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Custom transformer for dense conversion\n",
    "class DenseTransformer(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        if issparse(X):\n",
    "            return X.toarray()\n",
    "        return X\n",
    "\n",
    "# Custom transformer for date-time feature extraction\n",
    "class DateFeaturesExtractor(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        X['hour'] = X['date_time'].dt.hour\n",
    "        X['day'] = X['date_time'].dt.day\n",
    "        X['month'] = X['date_time'].dt.month\n",
    "        X['year'] = X['date_time'].dt.year\n",
    "        return X[['hour', 'day', 'month', 'year']]\n",
    "\n",
    "\n",
    "# Separate features and target\n",
    "features = data.drop('traffic_volume', axis=1)\n",
    "target = data['traffic_volume']\n",
    "\n",
    "# Define columns for transformations\n",
    "numerical_cols = ['temp', 'rain_1h', 'snow_1h', 'clouds_all']\n",
    "categorical_cols = ['holiday', 'weather_main', 'weather_description']\n",
    "\n",
    "# Column transformer with all preprocessing steps to normalize the numerical features, create one hot encoder for categorical features, and break up date time feature\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', MinMaxScaler(), numerical_cols),\n",
    "        ('cat', OneHotEncoder(), categorical_cols),\n",
    "        ('date', DateFeaturesExtractor(), ['date_time'])\n",
    "    ], remainder='drop')\n",
    "\n",
    "# Create the preprocessing pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('to_dense', DenseTransformer())\n",
    "])\n",
    "\n",
    "# Applying the pipeline to the feature data\n",
    "transformed_features = pipeline.fit_transform(features)\n",
    "\n",
    "# Fetch feature names from the OneHotEncoder and concatenate with other feature names\n",
    "feature_names = np.concatenate([\n",
    "    numerical_cols,\n",
    "    pipeline.named_steps['preprocessor'].transformers_[1][1].get_feature_names_out(categorical_cols),\n",
    "    ['hour', 'day', 'month', 'year']\n",
    "])\n",
    "\n",
    "# Create DataFrame from the processed features\n",
    "transformed_df = pd.DataFrame(transformed_features, columns=feature_names)\n",
    "transformed_df['traffic_volume'] = target  # Adding the target variable back\n",
    "\n",
    "# Display the transformed data\n",
    "transformed_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the **date_time variable**, you typically extract features that could have predictive power, such as the hour of the day, day of the week, month, or even year if the dataset spans several years. These extracted features can then be treated as categorical or numerical data, as shown in the DateFeaturesExtractor transformer above.\n",
    "\n",
    "By using this pipeline, when new data comes in, you simply pass it through the pipeline which will handle all the preprocessing and prediction steps in one go. This approach not only maintains data integrity but also simplifies deployment and maintenance of your machine learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the Transformed Data for Machine Learning Application (Next Module's Focus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_df.to_csv('data/transformed_Metro_traffic_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1- Download the Traffic Data from the link mentioned in this Notebook.\n",
    "#### 2- Try to add custom noise and outliers to the data.\n",
    "#### 3- Repeat the data preparation and cleaning steps on this Notebook.\n",
    "#### 4- Create **Pair Plots** and **Correlation Heatmaps** from the features and the target to have more understanding of the underlying patterns in the dataset\n",
    "#### 5- Create a data transformation pipeline and save your dataset\n",
    "#### 6- Read and watch videos about Supervised Machine Learning - Regression to have a base understanding of how we could make predictions on this dataset\n",
    "#### 7- Bonus: Try creating Training, Validation, and Test Datasets from the Transformed dataset for the Machine Learning Pipeline (To be discussed in the next module) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
